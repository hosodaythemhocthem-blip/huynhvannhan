import { GoogleGenerativeAI, GenerativeModel } from "@google/generative-ai";

/* =========================================================
    üì¶ KHO INTERFACE
========================================================= */
export interface GradeResult {
  score: number;
  feedback: string;
  suggestions: string;
}

export interface ExamQuestion {
  text: string;
  type: string;
  options: string[];
  correctAnswer: string;
  explanation: string | null;
  points: number;
}

export interface ParsedExam {
  title: string;
  description: string;
  questions: ExamQuestion[];
}

/* =========================================================
    üîê L·∫§Y API KEY AN TO√ÄN
========================================================= */
const getApiKey = (): string => {
  // ∆Øu ti√™n l·∫•y t·ª´ Next.js Environment
  const nextKey = process.env.NEXT_PUBLIC_GEMINI_API_KEY;
  if (nextKey) return nextKey;

  // L·∫•y t·ª´ Vite Environment (n·∫øu d√πng Vite)
  const viteKey = (import.meta as any).env?.VITE_GEMINI_API_KEY;
  if (viteKey) return viteKey;

  return "";
};

/* =========================================================
    üß† FACTORY & CACHE MODEL
========================================================= */
const modelCache = new Map<string, GenerativeModel>();

const getModel = (isJson: boolean = false, temperature: number = 0.7): GenerativeModel => {
  const apiKey = getApiKey();
  if (!apiKey) {
    throw new Error("‚ùå GEMINI_API_KEY kh√¥ng t·ªìn t·∫°i trong file .env");
  }

  const cacheKey = `${isJson ? "json" : "text"}-${temperature}`;
  
  if (modelCache.has(cacheKey)) {
    return modelCache.get(cacheKey)!;
  }

  const genAI = new GoogleGenerativeAI(apiKey);
  
  // C·∫§U H√åNH QUAN TR·ªåNG: ƒê·∫£m b·∫£o model ID chu·∫©n x√°c ƒë·ªÉ tr√°nh 404
  const modelName = "gemini-1.5-flash"; 

  const model = genAI.getGenerativeModel({
    model: modelName,
  }, {
    // √âp ki·ªÉu generationConfig ƒë·ªÉ tr√°nh l·ªói truy·ªÅn tr·ª±c ti·∫øp v√†o getGenerativeModel ·ªü m·ªôt s·ªë phi√™n b·∫£n SDK
    apiVersion: "v1beta" 
  });

  // G√°n c·∫•u h√¨nh tr·ª±c ti·∫øp v√†o instance model
  (model as any).generationConfig = {
    temperature,
    ...(isJson ? { responseMimeType: "application/json" } : {}),
  };

  modelCache.set(cacheKey, model);
  return model;
};

/* =========================================================
    üßπ L√ÄM S·∫†CH JSON
========================================================= */
const cleanAndParseJSON = <T>(text: string): T => {
  try {
    const cleaned = text
      .replace(/```json/gi, "")
      .replace(/```/g, "")
      .trim();
    return JSON.parse(cleaned) as T;
  } catch (error) {
    console.error("‚ùå L·ªói parse JSON:", text);
    throw new Error("D·ªØ li·ªáu AI tr·∫£ v·ªÅ kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng JSON.");
  }
};

/* =========================================================
    üîÅ AUTO RETRY
========================================================= */
const delay = (ms: number) => new Promise(res => setTimeout(res, ms));

const withRetry = async <T>(fn: () => Promise<T>, retries = 2, delayMs = 1500): Promise<T> => {
  try {
    return await fn();
  } catch (err: any) {
    if (retries <= 0) throw err;
    console.warn(`‚ö†Ô∏è ƒêang th·ª≠ l·∫°i... (C√≤n ${retries} l·∫ßn)`);
    await delay(delayMs);
    return await withRetry(fn, retries - 1, delayMs * 2); 
  }
};

/* =========================================================
    üöÄ GEMINI MAIN SERVICE
========================================================= */
export const geminiService = {
  
  // 1. ƒê·ªçc v√† ph√¢n t√≠ch ƒë·ªÅ thi t·ª´ vƒÉn b·∫£n
  async parseExamWithAI(text: string): Promise<ParsedExam | null> {
    if (!text.trim()) return null;
    const model = getModel(true, 0.1);

    const prompt = `B·∫°n l√† chuy√™n gia gi√°o d·ª•c. Tr√≠ch xu·∫•t n·ªôi dung sau th√†nh JSON:
    {
      "title": "T√™n ƒë·ªÅ",
      "description": "M√¥ t·∫£",
      "questions": [
        {
          "text": "C√¢u h·ªèi",
          "type": "multiple_choice",
          "options": ["A", "B", "C", "D"],
          "correctAnswer": "N·ªôi dung ƒë√°p √°n ƒë√∫ng",
          "explanation": "Gi·∫£i th√≠ch",
          "points": 1
        }
      ]
    }
    VƒÉn b·∫£n c·∫ßn x·ª≠ l√Ω: \n${text}`;

    const result = await withRetry(() => model.generateContent(prompt));
    return cleanAndParseJSON<ParsedExam>(result.response.text());
  },

  // 2. T·∫°o ƒë·ªÅ thi m·ªõi theo ch·ªß ƒë·ªÅ
  async generateExam(topic: string, grade: string, count = 10): Promise<ExamQuestion[]> {
    const model = getModel(true, 0.8);
    const prompt = `T·∫°o m·∫£ng JSON g·ªìm ${count} c√¢u h·ªèi tr·∫Øc nghi·ªám To√°n l·ªõp ${grade} v·ªÅ "${topic}". 
    M·ªói c√¢u c√≥: text, type, options (m·∫£ng 4 c√¢u), correctAnswer (text), explanation, points.`;

    const result = await withRetry(() => model.generateContent(prompt));
    return cleanAndParseJSON<ExamQuestion[]>(result.response.text());
  },

  // 3. Ch·∫•m ƒëi·ªÉm b√†i l√†m
  async gradeEssay(question: string, userAnswer: string): Promise<GradeResult> {
    const model = getModel(true, 0.3);
    const prompt = `Ch·∫•m ƒëi·ªÉm b√†i l√†m sau tr√™n thang 10. Tr·∫£ v·ªÅ JSON: {score, feedback, suggestions}.
    ƒê·ªÅ b√†i: ${question}
    B√†i l√†m: ${userAnswer}`;

    try {
      const result = await withRetry(() => model.generateContent(prompt));
      return cleanAndParseJSON<GradeResult>(result.response.text());
    } catch (error) {
      return { score: 0, feedback: "L·ªói k·∫øt n·ªëi AI.", suggestions: "Th·ª≠ l·∫°i sau." };
    }
  },

  // 4. Chat t·ª± do
  async chatWithAI(prompt: string): Promise<string> {
    const model = getModel(false, 0.7);
    try {
      const result = await withRetry(() => model.generateContent(prompt));
      return result.response.text();
    } catch (error) {
      return "AI ƒëang b·∫≠n, b·∫°n th·ª≠ l·∫°i sau nh√©!";
    }
  }
};
