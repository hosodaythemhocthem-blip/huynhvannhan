import { GoogleGenerativeAI, GenerativeModel } from "@google/generative-ai";

/* =========================================================
   üì¶ KHO INTERFACE (ƒê·ªãnh nghƒ©a ki·ªÉu d·ªØ li·ªáu chu·∫©n)
========================================================= */
export interface GradeResult {
  score: number;
  feedback: string;
  suggestions: string;
}

export interface ExamQuestion {
  text: string;
  type: string;
  options: string[];
  correctAnswer: string;
  explanation: string | null;
  points: number;
}

export interface ParsedExam {
  title: string;
  description: string;
  questions: ExamQuestion[];
}

/* =========================================================
   üîê L·∫§Y API KEY AN TO√ÄN (H·ªó tr·ª£ c·∫£ Vite & Next.js)
========================================================= */
const getApiKey = (): string => {
  if (typeof process !== "undefined" && process.env?.NEXT_PUBLIC_GEMINI_API_KEY) {
    return process.env.NEXT_PUBLIC_GEMINI_API_KEY;
  }
  if (typeof import.meta !== "undefined" && (import.meta as any).env?.VITE_GEMINI_API_KEY) {
    return (import.meta as any).env.VITE_GEMINI_API_KEY;
  }
  return "";
};

/* =========================================================
   üß† FACTORY & CACHE MODEL (T·ªëi ∆∞u RAM, t√°i s·ª≠ d·ª•ng Model)
========================================================= */
// D√πng Map ƒë·ªÉ l∆∞u l·∫°i c√°c model ƒë√£ kh·ªüi t·∫°o theo c·∫•u h√¨nh
const modelCache = new Map<string, GenerativeModel>();

const getModel = (isJson: boolean = false, temperature: number = 0.7): GenerativeModel => {
  const apiKey = getApiKey();
  if (!apiKey) {
    throw new Error("‚ùå Ch∆∞a c·∫•u h√¨nh GEMINI API KEY trong file .env");
  }

  // T·∫°o kh√≥a cache (VD: "json-0.1" ho·∫∑c "text-0.7")
  const cacheKey = `${isJson ? "json" : "text"}-${temperature}`;
  
  // N·∫øu ƒë√£ kh·ªüi t·∫°o model n√†y r·ªìi th√¨ l·∫•y ra d√πng lu√¥n (Si√™u nhanh)
  if (modelCache.has(cacheKey)) {
    return modelCache.get(cacheKey)!;
  }

  const genAI = new GoogleGenerativeAI(apiKey);
  
  // C·∫•u h√¨nh linh ho·∫°t
  const config: any = { temperature };
  if (isJson) config.responseMimeType = "application/json";

  const model = genAI.getGenerativeModel({
    model: "gemini-1.5-flash", // B·∫£n chu·∫©n ·ªïn ƒë·ªãnh nh·∫•t, kh√¥ng b·ªã l·ªói 404
    generationConfig: config,
  });

  // L∆∞u v√†o cache ƒë·ªÉ d√πng cho l·∫ßn sau
  modelCache.set(cacheKey, model);
  return model;
};

/* =========================================================
   üßπ L√ÄM S·∫†CH V√Ä √âP KI·ªÇU JSON CH·ªêNG L·ªñI
========================================================= */
const cleanAndParseJSON = <T>(text: string): T => {
  try {
    // Qu√©t s·∫°ch m·ªçi th·∫ª markdown (```json, ```html, ```) bao quanh
    const cleaned = text
      .replace(/```(?:json)?/gi, "")
      .replace(/```/g, "")
      .trim();

    return JSON.parse(cleaned) as T;
  } catch (error) {
    console.error("‚ùå L·ªói parse JSON t·ª´ chu·ªói AI tr·∫£ v·ªÅ:\n", text);
    throw new Error("AI tr·∫£ v·ªÅ d·ªØ li·ªáu kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng JSON chu·∫©n.");
  }
};

/* =========================================================
   üîÅ AUTO RETRY V·ªöI EXPONENTIAL BACKOFF (Ch·ªëng lag/Ch·ªëng spam)
========================================================= */
const delay = (ms: number) => new Promise(res => setTimeout(res, ms));

const withRetry = async <T>(
  fn: () => Promise<T>,
  retries = 2,
  delayMs = 1000 // Ch·ªù 1s r·ªìi m·ªõi th·ª≠ l·∫°i
): Promise<T> => {
  try {
    return await fn();
  } catch (err: any) {
    if (retries <= 0) throw err;
    console.warn(`‚ö†Ô∏è M·∫°ng l·ªói ho·∫∑c AI qu√° t·∫£i. ƒêang th·ª≠ l·∫°i... (C√≤n ${retries} l·∫ßn)`);
    await delay(delayMs);
    // L·∫ßn th·ª≠ l·∫°i ti·∫øp theo s·∫Ω ƒë·ª£i l√¢u h∆°n (2s, 4s...) ƒë·ªÉ server Google k·ªãp th·ªü
    return await withRetry(fn, retries - 1, delayMs * 2); 
  }
};

/* =========================================================
   üöÄ GEMINI MAIN SERVICE
========================================================= */
export const geminiService = {
  
  /* =============================
     1Ô∏è‚É£ ƒê·ªçc hi·ªÉu & Parse ƒë·ªÅ thi
  ============================== */
  async parseExamWithAI(text: string): Promise<ParsedExam | null> {
    if (!text.trim()) return null;

    // L·∫•y model c·∫•u h√¨nh JSON, temperature th·∫•p (0.1) ƒë·ªÉ AI c·ª±c k·ª≥ chu·∫©n x√°c
    const model = getModel(true, 0.1);

    const prompt = `
B·∫°n l√† chuy√™n gia gi√°o d·ª•c. Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√¥ sau th√†nh JSON chu·∫©n x√°c.

Y√™u c·∫ßu Output JSON:
{
  "title": "T√™n ƒë·ªÅ thi (Tr√≠ch xu·∫•t t·ª´ vƒÉn b·∫£n, m·∫∑c ƒë·ªãnh: ƒê·ªÅ thi m·ªõi)",
  "description": "M√¥ t·∫£ ng·∫Øn g·ªçn (n·∫øu c√≥)",
  "questions": [
    {
      "text": "N·ªôi dung c√¢u h·ªèi",
      "type": "multiple_choice",
      "options": ["ƒê√°p √°n 1", "ƒê√°p √°n 2", "ƒê√°p √°n 3", "ƒê√°p √°n 4"],
      "correctAnswer": "N·ªôi dung ƒë√°p √°n ƒë√∫ng",
      "explanation": "L·ªùi gi·∫£i chi ti·∫øt (n·∫øu c√≥, n·∫øu kh√¥ng ghi null)",
      "points": 1
    }
  ]
}

QUY T·∫ÆC NGHI√äM NG·∫∂T:
1. X√≥a ti·ªÅn t·ªë th·ª´a ·ªü c√¢u h·ªèi (VD: "C√¢u 1:", "B√†i 2:").
2. X√≥a ti·ªÅn t·ªë th·ª´a ·ªü ƒë√°p √°n (VD: "A.", "B.", "C.").
3. Gi·ªØ nguy√™n c√¥ng th·ª©c to√°n h·ªçc LaTeX trong c·∫∑p $...$ ho·∫∑c $$...$$.
4. Ch·ªâ l·∫•y th√¥ng tin c√≥ trong vƒÉn b·∫£n, KH√îNG T·ª∞ B·ªäA ƒê·∫∂T.
5. Ch·ªâ tr·∫£ v·ªÅ JSON thu·∫ßn t√∫y, kh√¥ng k√®m l·ªùi ch√†o.

VƒÉn b·∫£n:
"""
${text}
"""`;

    const result = await withRetry(() => model.generateContent(prompt));
    return cleanAndParseJSON<ParsedExam>(result.response.text());
  },

  /* =============================
     2Ô∏è‚É£ Sinh ƒë·ªÅ thi m·ªõi ng·∫´u nhi√™n
  ============================== */
  async generateExam(topic: string, grade: string, questionCount = 10): Promise<ExamQuestion[]> {
    // L·∫•y model c·∫•u h√¨nh JSON, temperature cao (0.7) ƒë·ªÉ AI s√°ng t·∫°o
    const model = getModel(true, 0.7);

    const prompt = `
ƒê√≥ng vai gi√°o vi√™n gi·ªèi, t·∫°o m·ªôt ƒë·ªÅ thi tr·∫Øc nghi·ªám m√¥n To√°n l·ªõp ${grade} v·ªÅ ch·ªß ƒë·ªÅ: "${topic}".
S·ªë l∆∞·ª£ng: ${questionCount} c√¢u. Y√™u c·∫ßu ƒë·ªô kh√≥ tƒÉng d·∫ßn.

Output JSON l√† M·∫¢NG c√¢u h·ªèi:
[
  {
    "text": "N·ªôi dung c√¢u (d√πng LaTeX trong $...$ cho c√¥ng th·ª©c)",
    "type": "multiple_choice",
    "options": ["T√πy ch·ªçn 1", "T√πy ch·ªçn 2", "T√πy ch·ªçn 3", "T√πy ch·ªçn 4"],
    "correctAnswer": "T√πy ch·ªçn ƒë√∫ng (ghi l·∫°i to√†n b·ªô text ƒë√°p √°n ƒë√∫ng)",
    "explanation": "Gi·∫£i th√≠ch b∆∞·ªõc gi·∫£i chi ti·∫øt",
    "points": 1
  }
]`;

    const result = await withRetry(() => model.generateContent(prompt));
    return cleanAndParseJSON<ExamQuestion[]>(result.response.text());
  },

  /* =============================
     3Ô∏è‚É£ Ch·∫•m ƒëi·ªÉm b√†i lu·∫≠n/t·ª± lu·∫≠n
  ============================== */
  async gradeEssay(question: string, userAnswer: string): Promise<GradeResult> {
    const model = getModel(true, 0.2);

    const prompt = `
B·∫°n l√† gi√°m kh·∫£o ch·∫•m thi.
C√¢u h·ªèi/ƒê·ªÅ b√†i: "${question}"
B√†i l√†m c·ªßa h·ªçc sinh: "${userAnswer}"

H√£y ch·∫•m ƒëi·ªÉm c√¥ng t√¢m tr√™n thang 10.
Output JSON:
{
  "score": 8.5,
  "feedback": "Nh·∫≠n x√©t chi ti·∫øt ∆∞u/khuy·∫øt ƒëi·ªÉm",
  "suggestions": "G·ª£i √Ω c√°ch l√†m b√†i t·ªët h∆°n"
}`;

    try {
      const result = await withRetry(() => model.generateContent(prompt));
      return cleanAndParseJSON<GradeResult>(result.response.text());
    } catch (error: any) {
      console.error("Gemini Grade Error:", error);
      return {
        score: 0,
        feedback: "H·ªá th·ªëng AI ƒëang qu√° t·∫£i, kh√¥ng th·ªÉ ch·∫•m b√†i l√∫c n√†y.",
        suggestions: "Vui l√≤ng t·∫£i l·∫°i trang ho·∫∑c th·ª≠ l·∫°i sau √≠t ph√∫t."
      };
    }
  },

  /* =============================
     4Ô∏è‚É£ Chat t·ª± do v·ªõi Tr·ª£ l√Ω
  ============================== */
  async chatWithAI(prompt: string): Promise<string> {
    // Chat th∆∞·ªùng th√¨ kh√¥ng d√πng JSON, temperature = 0.7 ƒë·ªÉ giao ti·∫øp t·ª± nhi√™n
    const model = getModel(false, 0.7);

    try {
      const result = await withRetry(() => model.generateContent(prompt));
      return result.response.text();
    } catch (error) {
      console.error("Gemini Chat Error:", error);
      return "Xin l·ªói b·∫°n, ƒë∆∞·ªùng truy·ªÅn ƒë·∫øn m√°y ch·ªß ƒëang g·∫∑p s·ª± c·ªë. B·∫°n nh·∫Øn l·∫°i sau m·ªôt l√°t nh√©!";
    }
  }
};
